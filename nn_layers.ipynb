{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_layers",
      "provenance": [],
      "authorship_tag": "ABX9TyM8kLJERwaUwdHu/4rV7u7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JIHEON-L/DFE604-2020F/blob/master/nn_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMFODZKqatBo"
      },
      "source": [
        "import numpy as np\n",
        "from skimage.util.shape import view_as_windows\n",
        "\n",
        "##########\n",
        "#   convolutional layer\n",
        "#   you can re-use your previous implementation, or modify it if necessary\n",
        "##########\n",
        "\n",
        "class nn_convolutional_layer:\n",
        "\n",
        "    def __init__(self, Wx_size, Wy_size, input_size, in_ch_size, out_ch_size, std=1e0):\n",
        "    \n",
        "        # initialization of weights\n",
        "        self.W = np.random.normal(0, std / np.sqrt(in_ch_size * Wx_size * Wy_size / 2),\n",
        "                                  (out_ch_size, in_ch_size, Wx_size, Wy_size))\n",
        "        self.b = 0.01 + np.zeros((1, out_ch_size, 1, 1))\n",
        "        self.input_size = input_size\n",
        "\n",
        "    def update_weights(self, dW, db):\n",
        "        self.W += dW\n",
        "        self.b += db\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.W, self.b\n",
        "\n",
        "    def set_weights(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "    def forward(self, x):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape=(batch size, input channel size, in width, in height)\n",
        "        # = (batch size, 1, 28, 28)\n",
        "        # self.W.shape = (num_filters, in_ch_size, filter_width, filter_height)\n",
        "        # = (28, 1, 3, 3)       # 각 배치마다 filter 한 개\n",
        "        # self.b.shape = (1, num_filters, 1, 1)\n",
        "        # = (1, 28, 1, 1)\n",
        "        \n",
        "        b = np.squeeze(self.b)\n",
        "\n",
        "        # (batch size, filter_num, outw, outh)\n",
        "        out = np.zeros(shape = (x.shape[0], self.W.shape[0], x.shape[2] - self.W.shape[2] + 1, x.shape[3] - self.W.shape[3] + 1)) \n",
        "        for i in range (x.shape[0]):        # 데이터 수\n",
        "            y = view_as_windows(x[i], (self.W.shape[1], self.W.shape[2], self.W.shape[3]))  # (채널, 가로, 세로), \n",
        "            y = y.reshape((y.shape[1], y.shape[2], -1))     # (30, 30, 27)\n",
        "\n",
        "            out_f = np.zeros(shape = (out.shape[1], out.shape[2], out.shape[3]))    # (8, 30, 30)\n",
        "            \n",
        "            for j in range (self.W.shape[0]):       # 필터 수\n",
        "                out_ins = y.dot(self.W[j].reshape((-1, 1)))     # (30, 30, 1)       # reshape한 W = (27, 1)\n",
        "                out_ins = np.squeeze(out_ins,axis=2)        # (30, 30)\n",
        "                out_f[j] = out_ins + b[j]       # bias 더하기\n",
        "            \n",
        "            out[i] = out_f\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backprop(self, x, dLdy):\n",
        "\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 1, 28, 28)\n",
        "        # self.b.shape = (1, 28, 1, 1)\n",
        "        # self.W.shape = (28, 1, 3, 3)\n",
        "        # dLdy.shape = (50, 28, 26, 26)\n",
        "\n",
        "        bat_siz, num_f, out_w, out_h = dLdy.shape\n",
        "        _, depth, fil_w, fil_h = self.W.shape   \n",
        "\n",
        "        W_flip = np.zeros(self.W.shape)     # (28, 1, 3, 3)                                     \n",
        "        for i in range (num_f):     # W 상하좌우 뒤집기\n",
        "            for j in range (depth):\n",
        "                W_flip[i, j] = np.flip(self.W[i, j])\n",
        "\n",
        "        n = x.shape[2] - (dLdy.shape[2])        # 28 - 26 = 2\n",
        "        dLdy_pad = np.zeros(shape = (bat_siz, num_f, out_w + 2 * n, out_h + 2 * n))     # (50, 28, 30, 30)\n",
        "        for i in range (bat_siz):\n",
        "            for j in range (num_f):\n",
        "                dLdy_pad[i, j] = np.pad(dLdy[i, j], ((n, n), (n, n)), 'constant', constant_values = 0)\n",
        "        \n",
        "        dLdx = np.zeros(x.shape)\n",
        "        for i in range(bat_siz):\n",
        "            for f in range (num_f):\n",
        "                for d in range (depth):\n",
        "                    w = view_as_windows(dLdy_pad[i, f], (fil_w, fil_h))               # 여기선 filter가 W_flip\n",
        "                    w = w.reshape((w.shape[0], w.shape[1], -1))\n",
        "                    \n",
        "                    dLdx_ins = w.dot(W_flip[f, d].reshape((-1, 1)))\n",
        "                    dLdx_ins = np.squeeze(dLdx_ins)         \n",
        "                    dLdx[i, d] += dLdx_ins\n",
        "\n",
        "        dLdW = np.zeros(self.W.shape)\n",
        "        for i in range (bat_siz):\n",
        "            for f in range (num_f):\n",
        "                for d in range (depth):\n",
        "                    #dLdW[f, d] += x[i, d] * dLdy[i, f]\n",
        "                    w = view_as_windows(x[i, d], (out_w, out_h))        \n",
        "                    w = w.reshape((w.shape[0], w.shape[1], -1))         \n",
        "\n",
        "                    dLdW_ins = w.dot(dLdy[i, f].reshape((-1, 1)))       \n",
        "                    dLdW_ins = np.squeeze(dLdW_ins)                     \n",
        "                    dLdW[f, d] += dLdW_ins\n",
        "\n",
        "        dLdb = np.sum(dLdy, axis = (0, 2, 3)) \n",
        "        dLdb = dLdb.reshape(self.b.shape)\n",
        "\n",
        "        return dLdx, dLdW, dLdb\n",
        "\n",
        "##########\n",
        "#   max pooling layer\n",
        "#   you can re-use your previous implementation, or modify it if necessary\n",
        "##########\n",
        "\n",
        "class nn_max_pooling_layer:\n",
        "    def __init__(self, stride, pool_size):\n",
        "        self.stride = stride\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 28, 26, 26)\n",
        "        # (batch size, num_filter, w / strid, h / strid)\n",
        "        out = np.zeros(shape = (x.shape[0], x.shape[1], x.shape[2]//2, x.shape[3]//2))\n",
        "        \n",
        "        for i in range (x.shape[0]):    # 각 데이터 마다\n",
        "            out_f = np.zeros(shape = (out.shape[1], out.shape[2], out.shape[3]))\n",
        "            \n",
        "            for j in range (x.shape[1]):    # 각 depth 마다\n",
        "                y = view_as_windows(x[i][j], (self.pool_size, self.pool_size), step = self.stride)      # y = (13, 13, 2, 2)\n",
        "                max_p = np.max(y, axis=(2, 3))  # 작은 (2, 2)중 가장 큰 거\n",
        "                out_f[j] = max_p\n",
        "            \n",
        "            out[i] = out_f\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backprop(self, x, dLdy):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 28, 26, 26)\n",
        "        # dLdy.shape = (50, 1, 4732)\n",
        "        \n",
        "        dLdy = dLdy.reshape(x.shape[0], x.shape[1], x.shape[3]//2, x.shape[2]//2)\n",
        "\n",
        "        dLdx = np.zeros(x.shape)    \n",
        "        \n",
        "        for i in range (dLdx.shape[0]):     # bacth 사이즈 만큼 반복 >> for문 안에서는 (3, 32, 32)\n",
        "            dLdx_i = np.zeros(shape = (dLdx.shape[1], dLdx.shape[2], dLdx.shape[3]))        # (3, 32, 32)\n",
        "\n",
        "            for j in range (dLdx.shape[1]):     # channel 갯수 만큼 반복 >> for문 안에서는 한 장! (32, 32)\n",
        "\n",
        "                # max index를 알아내서, 그 부분은 dLdy값을, 다른 부분은 0을!\n",
        "                y = view_as_windows(x[i][j], (self.pool_size, self.pool_size), step = self.stride)      # (16, 16, 2, 2) - (2, 2)크기 만큼 (16, 16)개\n",
        "                y = y.reshape(y.shape[0], y.shape[1], -1)   # (16, 16, 4)\n",
        "                \n",
        "                dLdx_j = np.zeros(shape = (dLdx.shape[2], dLdx.shape[3]))       # (32, 32)\n",
        "\n",
        "                for y1 in range(y.shape[0]):        # 16번 바복\n",
        "                    for y2 in range(y.shape[1]):    # 16번 반복\n",
        "                        max_idx = y[y1, y2].argmax()\n",
        "                        \n",
        "                        for y3 in range (y.shape[2]):   # 4번 이하 반복\n",
        "                            if y3 == max_idx:\n",
        "                                col = 2 * y2 + (y3 % 2)\n",
        "                                row = 2 * y1 + (y3 % 2)\n",
        "                                dLdx_j[row, col] = dLdy[i, j, y1, y2]\n",
        "                                break\n",
        "                \n",
        "                dLdx_i[j] = dLdx_j\n",
        "\n",
        "            dLdx[i] = dLdx_i       \n",
        "\n",
        "        return dLdx\n",
        "\n",
        "\n",
        "\n",
        "##########\n",
        "#   fully connected layer\n",
        "##########\n",
        "# fully connected linear layer.\n",
        "# parameters: weight matrix matrix W and bias b\n",
        "# forward computation of y=Wx+b\n",
        "# for (input_size)-dimensional input vector, outputs (output_size)-dimensional vector\n",
        "# x can come in batches, so the shape of y is (batch_size, output_size)\n",
        "# W has shape (output_size, input_size), and b has shape (output_size,)\n",
        "\n",
        "class nn_fc_layer:\n",
        "\n",
        "    def __init__(self, input_size, output_size, std=1):\n",
        "        # Xavier/He init\n",
        "        self.W = np.random.normal(0, std/np.sqrt(input_size/2), (output_size, input_size))\n",
        "        self.b=0.01+np.zeros((output_size))\n",
        "\n",
        "    def forward(self,x):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 28, 13, 13)\n",
        "        # self.W.shape = (output_size, input_size) = (128, 4732)\n",
        "        # self.b.shape = (output_size, )\n",
        "\n",
        "        #print(\"1\")\n",
        "        b = self.b.reshape(-1, 1)       # (128, 1)  / (10, 1)\n",
        "        #print(\"2\")\n",
        "        xre = x.reshape(x.shape[0], -1, 1)      # (50, 4732, 1) / (50, 128, 1)\n",
        "        #print(\"3\")\n",
        "        # (50, 128, 1)  / (50, 10, 1)\n",
        "        out = self.W @ xre + b\n",
        "        #print(\"4\")\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backprop(self,x,dLdy):\n",
        "\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # dLdy = (50, 1, 10) / (50, 1, 128)\n",
        "        # x = (50, 128, 1) / (50, 28, 13, 13)\n",
        "        # W = (10, 128) / (128, 4732)\n",
        "        # b = (10, ) /  (128, )\n",
        "\n",
        "        b = self.b.reshape(-1, 1)\n",
        "        x = x.reshape(x.shape[0], -1, 1)    # batch size만 유지하고, 펼치기\n",
        "\n",
        "        # (50, 1, 128) / (50, 1, 4732)\n",
        "        dLdx = dLdy @ self.W\n",
        "        \n",
        "        # (1, 10) / (1, 128)\n",
        "        dLdb = np.sum(dLdy @ np.eye(b.shape[0]), axis=0) / x.shape[0]\n",
        "\n",
        "        # (10, 128) / (128, 4732)\n",
        "        dLdW = np.sum((dLdy.reshape(dLdy.shape[0], dLdy.shape[2], dLdy.shape[1])) @ x.reshape(x.shape[0], x.shape[2], x.shape[1]), axis=0) / x.shape[0]\n",
        "\n",
        "        return dLdx,dLdW,dLdb\n",
        "\n",
        "    def update_weights(self,dLdW,dLdb):\n",
        "\n",
        "        # parameter update\n",
        "        self.W=self.W+dLdW\n",
        "        self.b=self.b+dLdb\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.W, self.b\n",
        "\n",
        "    def set_weights(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "##########\n",
        "#   activation layer\n",
        "##########\n",
        "#   This is ReLU activation layer.\n",
        "##########\n",
        "\n",
        "class nn_activation_layer:\n",
        "    \n",
        "    # performs ReLU activation\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "        \n",
        "        # x.shape = (50, 28, 26, 26)\n",
        "\n",
        "        out = np.where(x < 0, 0, x)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backprop(self, x, dLdy):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "        \n",
        "        # x.shape = (50, 128, 1) / (50, 28, 26, 26)\n",
        "        # dLdy.shape = (50, 1, 128) / (50, 28, 26, 26)\n",
        "        \n",
        "        # (50, 1, 128) / \n",
        "\n",
        "        x_shape = x.shape\n",
        "        dLdy_shape = dLdy.shape\n",
        "        \n",
        "        x = x.reshape(-1, 1)        # 쭉 펼치기\n",
        "        dLdy = dLdy.reshape(-1, 1)\n",
        "    \n",
        "        for i in range (x.shape[0]):\n",
        "            if x[i][0] <= 0:\n",
        "                dLdy[i][0] = 0\n",
        "        \n",
        "        dLdx = dLdy.reshape(dLdy_shape)\n",
        "\n",
        "        return dLdx\n",
        "\n",
        "\n",
        "##########\n",
        "#   softmax layer\n",
        "#   you can re-use your previous implementation, or modify it if necessary\n",
        "##########\n",
        "\n",
        "class nn_softmax_layer:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 10, 1)\n",
        "\n",
        "        #max_x = np.max(x)\n",
        "        #exp = np.exp(x - max_x)\n",
        "        exp = np.exp(x)\n",
        "        exp_sum = exp.sum(axis=1)\n",
        "        \n",
        "        out = np.zeros(x.shape)\n",
        "        for i in range (x.shape[0]):\n",
        "            for j in range (x.shape[1]):\n",
        "                out[i][j] = np.exp(x[i][j]) / exp_sum[i]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backprop(self, x, dLdy):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # dLdy = (50, 1, 10)\n",
        "        # x.shape = (50, 10, 1)\n",
        "        \n",
        "        #max_x = np.max(x)\n",
        "        #exp = np.exp(x - max_x)     \n",
        "        exp = np.exp(x)\n",
        "        exp_sum = exp.sum(axis=1)\n",
        "\n",
        "        # (50, 10, 10)\n",
        "        dydx = np.zeros((x.shape[0], dLdy.shape[2], x.shape[1]))\n",
        "        for k in range (x.shape[0]):\n",
        "            for i in range (dydx.shape[1]): # y\n",
        "                for j in range (dydx.shape[2]): # x\n",
        "                    if i==j:\n",
        "                        dydx[k][i][j] = (np.exp(x[k][i]) / exp_sum[k]) * (1 - (np.exp(x[k][i]) / exp_sum[k]))\n",
        "                    else:\n",
        "                        dydx[k][i][j] = - (np.exp(x[k][i]) / exp_sum[k]) * (np.exp(x[k][j]) / exp_sum[k])\n",
        "\n",
        "\n",
        "        dLdx = dLdy @ dydx\n",
        "\n",
        "        return dLdx\n",
        "\n",
        "##########\n",
        "#   cross entropy layer\n",
        "#   you can re-use your previous implementation, or modify it if necessary\n",
        "##########\n",
        "\n",
        "class nn_cross_entropy_layer:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # y는 train label\n",
        "        # x = (50, 10, 1)\n",
        "        y = y.reshape(-1, 1)\n",
        "        \n",
        "        loss_sum = 0\n",
        "        for i in range(x.shape[0]):\n",
        "            label = y[i]\n",
        "            loss_sum += np.log(x[i][label])\n",
        "        \n",
        "        out = - loss_sum/x.shape[0]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        ##########\n",
        "        ##########\n",
        "        #   Complete the method with your implementation\n",
        "        ##########\n",
        "        ##########\n",
        "\n",
        "        # x.shape = (50, 10, 1)\n",
        "        # x는 softmax 결과 값\n",
        "        dLdx = np.zeros((x.shape[0], x.shape[2], x.shape[1]))\n",
        "\n",
        "        for i in range (x.shape[0]):\n",
        "            label = y[i]\n",
        "            dLdx[i][0][label] = -1 / x[i][label]\n",
        "            \n",
        "        return dLdx"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}